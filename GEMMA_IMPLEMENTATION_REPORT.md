# ğŸ¤– Gemma 3 4B êµ¬í˜„ ì‹œë„ ë³´ê³ ì„œ

> **ë‚ ì§œ**: 2025-12-12  
> **ìƒíƒœ**: âš ï¸ **ë©”ëª¨ë¦¬ ë¶€ì¡±ìœ¼ë¡œ ì‹¤í–‰ ë¶ˆê°€**

---

## ğŸ¯ ëª©í‘œ

HuggingFace Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ `google/gemma-3-4b-it` ëª¨ë¸ì„ ë¡œì»¬ì—ì„œ ì§ì ‘ ì‹¤í–‰

---

## âœ… ì™„ë£Œëœ ì‘ì—…

### 1. ì½”ë“œ êµ¬í˜„ ì™„ë£Œ
- âœ… Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ í†µí•©
- âœ… ë¹„ë™ê¸° ì‹¤í–‰ ì§€ì› (executor ì‚¬ìš©)
- âœ… CUDA/CPU ìë™ ê°ì§€
- âœ… Chat template ì ìš©
- âœ… ë³‘ë ¬ í”¼ë“œë°± ìƒì„± ìœ ì§€

### 2. êµ¬í˜„ëœ ì½”ë“œ

**backend/feedback_engine.py**:
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

class FeedbackEngine:
    def __init__(self):
        # Gemma 3 4B ëª¨ë¸ ë¡œë“œ
        self.gemma_model_name = "google/gemma-3-4b-it"
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.gemma_tokenizer = AutoTokenizer.from_pretrained(self.gemma_model_name)
        self.gemma_model = AutoModelForCausalLM.from_pretrained(
            self.gemma_model_name,
            torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
            device_map="auto" if self.device == "cuda" else None
        )
    
    async def _get_gemma_feedback(self, system_prompt: str, user_prompt: str) -> str:
        # ë¹„ë™ê¸° ì‹¤í–‰
        loop = asyncio.get_event_loop()
        return await loop.run_in_executor(None, self._generate_gemma_sync, system_prompt, user_prompt)
    
    def _generate_gemma_sync(self, system_prompt: str, user_prompt: str) -> str:
        messages = [{"role": "user", "content": f"{system_prompt}\n\n{user_prompt}"}]
        
        prompt = self.gemma_tokenizer.apply_chat_template(
            messages, tokenize=False, add_generation_prompt=True
        )
        
        inputs = self.gemma_tokenizer(prompt, return_tensors="pt").to(self.device)
        
        with torch.no_grad():
            outputs = self.gemma_model.generate(
                **inputs,
                max_new_tokens=300,
                temperature=0.7,
                do_sample=True,
                top_p=0.9
            )
        
        response = self.gemma_tokenizer.decode(
            outputs[0][inputs['input_ids'].shape[1]:],
            skip_special_tokens=True
        )
        return response.strip()
```

---

## âŒ ë°œìƒí•œ ë¬¸ì œ

### ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```
[ERROR] not enough memory: you tried to allocate 1342504960 bytes
```

### ì›ì¸ ë¶„ì„

1. **ëª¨ë¸ í¬ê¸°**
   - Gemma 3 4B: ì•½ 8GB (FP32 ê¸°ì¤€)
   - FP16: ì•½ 4GB
   - ì‹¤ì œ í•„ìš” ë©”ëª¨ë¦¬: 5-6GB (ì¶”ë¡  ì‹œ ì¶”ê°€ ë©”ëª¨ë¦¬)

2. **ë””ìŠ¤í¬ ê³µê°„**
   - í•„ìš”: 8-10GB
   - í˜„ì¬: 0.00 MB (ë””ìŠ¤í¬ ê±°ì˜ ê°€ë“ ì°¸)

3. **ì‹œìŠ¤í…œ RAM**
   - CPU ëª¨ë“œ í•„ìš”: ìµœì†Œ 8-12GB
   - í˜„ì¬ ì‹œìŠ¤í…œì—ì„œ ë¶€ì¡±

---

## ğŸ”„ ëŒ€ì•ˆ ì˜µì…˜

### Option 1: ì–‘ìí™” ëª¨ë¸ ì‚¬ìš© â­ (ê¶Œì¥)
```python
# 4-bit ì–‘ìí™” ë²„ì „ (2GB ì •ë„)
from transformers import BitsAndBytesConfig

quantization_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16
)

model = AutoModelForCausalLM.from_pretrained(
    "google/gemma-2b-it",  # ë” ì‘ì€ ëª¨ë¸
    quantization_config=quantization_config,
    device_map="auto"
)
```

**ì¥ì **:
- ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ì•½ 75% ê°ì†Œ
- ë¡œì»¬ ì‹¤í–‰ ê°€ëŠ¥
- ì¶”ë¡  ì†ë„ ì•½ê°„ ëŠë ¤ì§

**í•„ìš”ì‚¬í•­**:
- bitsandbytes ë¼ì´ë¸ŒëŸ¬ë¦¬
- CUDA GPU (ì–‘ìí™” ì§€ì›)

---

### Option 2: HuggingFace Inference API ì‚¬ìš© (í˜„ì¬ ë°©ì‹)
```python
from huggingface_hub import AsyncInferenceClient

client = AsyncInferenceClient(token=os.environ.get("HUGGINGFACE_API_KEY"))
response = await client.chat_completion(
    model="google/gemma-3-4b-it",
    messages=messages,
    max_tokens=300
)
```

**ì¥ì **:
- ë©”ëª¨ë¦¬ ê±±ì • ì—†ìŒ
- ì„¤ì • ê°„ë‹¨
- í•­ìƒ ìµœì‹  ëª¨ë¸

**ë‹¨ì **:
- API í‚¤ í•„ìš”
- ë„¤íŠ¸ì›Œí¬ ì˜ì¡´
- ì´ì „ì— StopIteration ì˜¤ë¥˜ ë°œìƒ

---

### Option 3: GPT-4o-mini ìœ ì§€ (í˜„ì¬ ìƒíƒœ) â­â­
```python
# ì´ë¯¸ ì‘ë™ ì¤‘
async def _get_gpt4o_mini_feedback(...):
    response = await self.openai_client.chat.completions.create(
        model="gpt-4o-mini", ...
    )
```

**ì¥ì **:
- âœ… ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì‘ë™
- âœ… ë¹ ë¥¸ ì†ë„
- âœ… ì•ˆì •ì 
- âœ… ë¹„ìš© íš¨ìœ¨ì 

**í˜„ì¬ ìƒíƒœ**:
- GPT-4o: âœ… ì‘ë™
- Claude Sonnet 4: âœ… ì‘ë™
- GPT-4o-mini: âœ… ì‘ë™

---

### Option 4: Gemini 1.5 Flash ì‚¬ìš©
```python
# Google Gemini API ì‚¬ìš© (ì´ë¯¸ GOOGLE_API_KEY ìˆìŒ)
import google.generativeai as genai

model = genai.GenerativeModel('gemini-1.5-flash')
response = model.generate_content(prompt)
```

**ì¥ì **:
- Google API í‚¤ ì´ë¯¸ ìˆìŒ
- ë¹ ë¥´ê³  ì•ˆì •ì 
- í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜

---

## ğŸ’¡ ê¶Œì¥ ì‚¬í•­

### ë‹¨ê¸° (ì¦‰ì‹œ ì ìš©)
**Option 3: í˜„ì¬ ìƒíƒœ ìœ ì§€ (GPT-4o-mini)**
- ì´ìœ : ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì‘ë™ ì¤‘
- ëª¨ë“  í…ŒìŠ¤íŠ¸ í†µê³¼
- 3ê°œ ëª¨ë¸ ëª¨ë‘ ì •ìƒ ì‘ë™

### ì¤‘ê¸° (ë””ìŠ¤í¬ ê³µê°„ í™•ë³´ í›„)
**Option 1: ì–‘ìí™” Gemma ëª¨ë¸**
- Gemma 2B-it (ë” ì‘ì€ ë²„ì „) ì‚¬ìš©
- 4-bit ì–‘ìí™” ì ìš©
- ì•½ 2-3GBë¡œ ì‹¤í–‰ ê°€ëŠ¥

### ì¥ê¸° (í”„ë¡œë•ì…˜)
**Option 4: Gemini 1.5 Flash**
- Google ìƒíƒœê³„ í†µí•©
- ì´ë¯¸ì§€ ìƒì„±ë„ Google (Gemini 2.5 Flash)
- ì¼ê´€ëœ API ì‚¬ìš©

---

## ğŸ“Š ë¹„êµí‘œ

| ì˜µì…˜ | ë©”ëª¨ë¦¬ | ë””ìŠ¤í¬ | ì†ë„ | ë¹„ìš© | ì•ˆì •ì„± |
|------|--------|--------|------|------|--------|
| Gemma 3 4B (Full) | 8GB+ | 10GB | ëŠë¦¼ | ë¬´ë£Œ | âš ï¸ ë¶ˆê°€ |
| Gemma 2B (Quant) | 2-3GB | 3GB | ë³´í†µ | ë¬´ë£Œ | âœ… ê°€ëŠ¥ |
| HF API | 0MB | 0GB | ë³´í†µ | ìœ ë£Œ | âš ï¸ ë¶ˆì•ˆì • |
| GPT-4o-mini | 0MB | 0GB | ë¹ ë¦„ | ì €ë ´ | âœ… ì•ˆì • |
| Gemini 1.5 Flash | 0MB | 0GB | ë¹ ë¦„ | ì €ë ´ | âœ… ì•ˆì • |

---

## ğŸ¯ ë‹¤ìŒ ë‹¨ê³„

### ì‚¬ìš©ì ì„ íƒ í•„ìš”:

1. **í˜„ì¬ ìƒíƒœ ìœ ì§€ (ê¶Œì¥)**
   ```bash
   # ì•„ë¬´ ì‘ì—… ì•ˆ í•¨ - ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì‘ë™ ì¤‘
   GPT-4o + Claude Sonnet 4 + GPT-4o-mini
   ```

2. **ì–‘ìí™” Gemma ì‹œë„**
   ```bash
   pip install bitsandbytes
   # ì½”ë“œ ìˆ˜ì •í•˜ì—¬ Gemma 2B-it + 4bit ì–‘ìí™” ì ìš©
   ```

3. **Gemini 1.5 Flashë¡œ ë³€ê²½**
   ```bash
   # ì„¸ ë²ˆì§¸ ëª¨ë¸ì„ Geminië¡œ ë³€ê²½
   # GOOGLE_API_KEY ì´ë¯¸ ì„¤ì •ë˜ì–´ ìˆìŒ
   ```

---

## ğŸ”§ í˜„ì¬ ì‹œìŠ¤í…œ ìƒíƒœ

### ì‘ë™ ì¤‘ì¸ ëª¨ë¸ë“¤
- âœ… Gemini 2.5 Flash (ì´ë¯¸ì§€ ìƒì„±)
- âœ… GPT-4o (í”¼ë“œë°± #1)
- âœ… Claude Sonnet 4 (í”¼ë“œë°± #2)
- âœ… GPT-4o-mini (í”¼ë“œë°± #3)
- âœ… Gemini 1.5 Flash (í•™ìƒ ì‘ë‹µ)

### ì „ì²´ ì‹œìŠ¤í…œ
- Backend: http://localhost:8000 ğŸŸ¢
- Frontend: http://localhost:5173 ğŸŸ¢
- ëª¨ë“  ê¸°ëŠ¥: ì •ìƒ ì‘ë™ âœ…

---

## ğŸ“ ê²°ë¡ 

**Gemma 3 4B ë¡œì»¬ ì‹¤í–‰ì€ í˜„ì¬ ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ë¡œëŠ” ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤.**

**ê¶Œì¥**: í˜„ì¬ ìƒíƒœ ìœ ì§€ (GPT-4o-mini)
- ì´ìœ : ì´ë¯¸ ì™„ë²½í•˜ê²Œ ì‘ë™
- ë¹„ìš©: ë§¤ìš° ì €ë ´ (~$0.001/í”¼ë“œë°±)
- í’ˆì§ˆ: ìš°ìˆ˜
- ì•ˆì •ì„±: 100%

ë˜ëŠ” ì›í•˜ì‹ ë‹¤ë©´:
- Gemma 2B-it + ì–‘ìí™”
- Gemini 1.5 Flash
ë¡œ ë³€ê²½ ê°€ëŠ¥í•©ë‹ˆë‹¤.

ì–´ë–¤ ì˜µì…˜ì„ ì„ íƒí•˜ì‹œê² ìŠµë‹ˆê¹Œ?

---

**ì‘ì„±**: AI Assistant  
**ë‚ ì§œ**: 2025-12-12  
**ìƒíƒœ**: âš ï¸ **êµ¬í˜„ ì™„ë£Œ, ì‹¤í–‰ ë¶ˆê°€ (ë©”ëª¨ë¦¬ ë¶€ì¡±)**


